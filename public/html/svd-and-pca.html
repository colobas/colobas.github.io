<h1 id="section"></h1>
<p>This is a two-part post, with some notes I wrote while refreshing the concepts of SVD and PCA.</p>
<h1 id="singular-value-decomposition">Singular Value Decomposition</h1>
<h2 id="section-1"></h2>
<p>Take any matrix <span class="math inline"><em>A</em></span>. Generally speaking, what it does to a vector <span class="math inline"><em>x⃗</em></span> is stretch it and rotate it.</p>
<h2 id="section-2"></h2>
<p>Now let's consider what a matrix <span class="math inline"><em>A</em></span> does to a unitary circle: <img src="file:///images/matrix-multiplication.png" /></p>
<h3 id="section-3"></h3>
<p>The orthonormal vectors <span class="math inline">$\vec{v_1}$</span> and <span class="math inline">$\vec{v_2}$</span> were rotated into the orthonormal vectors <span class="math inline">$\vec{u_1}$</span> and <span class="math inline">$\vec{u_2}$</span> and scaled by some constants, <span class="math inline"><em>σ</em><sub>1</sub></span> and <span class="math inline"><em>σ</em><sub>2</sub></span></p>
<h3 id="section-4"></h3>
<p>This is equivalent to saying <span class="math inline"><em>A</em><em>V</em> = <em>U</em><em>Σ</em></span>, where <span class="math inline"><em>V</em></span> has the vectors <span class="math inline"><em>v⃗</em></span> in its columns, and <span class="math inline"><em>U</em></span> has the vectors <span class="math inline"><em>u⃗</em></span> in its columns.</p>
<p>(Becaue <span class="math inline"><em>V</em></span> is orthonormal, its inverse is the same as its tranpose)</p>
<ol>
<li><p>This is called the reduced Singular Value Decomposition. More visually:</p>
<p><span class="math display">$$
\underbrace{\begin{bmatrix}
&amp; &amp; \\
&amp; A &amp;\\
&amp; &amp; \\
\end{bmatrix}}_{(m\times n)}
\
\underbrace{\begin{bmatrix}
\vert &amp; \vert &amp; &amp; \vert\\
\vec{v_1} &amp; \vec{v_2} &amp; ... &amp; \vec{v_n}\\
\vert &amp; \vert &amp; &amp; \vert\\
\end{bmatrix}}_{(n\times n)} =
\
\underbrace{\begin{bmatrix}
\vert &amp; \vert &amp; &amp; \vert\\
\vec{u_1} &amp; \vec{u_2} &amp; ... &amp; \vec{v_m}\\
\vert &amp; \vert &amp; &amp; \vert\\
\end{bmatrix}}_{(m\times n)}
\
\underbrace{\begin{bmatrix}
\sigma_1 &amp; &amp; \\
&amp; \ddots &amp; \\
&amp; &amp; \sigma_n \\
\end{bmatrix}}_{(n\times n)}
$$</span></p></li>
<li><p>Equivalently:</p>
<p><span class="math display">$$
\underbrace{\begin{bmatrix}
&amp; &amp; \\
&amp; &amp; \\
&amp; A &amp;\\
&amp; &amp; \\
&amp; &amp; \\
\end{bmatrix}}_{(m\times n)} = 
\
\underbrace{\begin{bmatrix}
&amp; &amp; \\
&amp; &amp; \\
&amp; \hat{U} &amp;\\
&amp; &amp; \\
&amp; &amp; \\
\end{bmatrix}}_{(m\times n)}
\
\underbrace{\begin{bmatrix}
\ &amp; &amp; \\
\ &amp; \hat{\Sigma} &amp;\\
\ &amp; &amp; \\
\end{bmatrix}}_{(n\times n)}
\
\underbrace{\begin{bmatrix}
&amp; &amp; \\
&amp; V^* &amp;\\
&amp; &amp; \\
\end{bmatrix}}_{(n\times n)}
$$</span></p></li>
<li><p>Since <span class="math inline"><em>Û</em></span> is underdetermined, normally we work with:</p></li>
<li><p>I.e., <span class="math inline"><em>U</em></span> is obtained by adding <span class="math inline">(<em>m</em> − <em>n</em>)</span> orthonormal columns to <span class="math inline"><em>Û</em></span>, and <span class="math inline"><em>Σ</em></span> is obtained by adding <span class="math inline">(<em>m</em> − <em>n</em>)</span> rows of zeros to <span class="math inline"><em>Σ̂</em></span></p></li>
<li><p>Computing the SVD, boils down to finding the eigendecompositions of <span class="math inline"><em>A</em><em>A</em><sup><em>T</em></sup></span> and <span class="math inline"><em>A</em><sup><em>T</em></sup><em>A</em></span>. The eigenvectors of <span class="math inline"><em>A</em><em>A</em><sup><em>T</em></sup></span> are <span class="math inline"><em>U</em></span>, and the eigenvectors of <span class="math inline"><em>A</em><sup><em>T</em></sup><em>A</em></span> are <span class="math inline"><em>V</em></span>. The eigenvalues of both <span class="math inline"><em>A</em><em>A</em><sup><em>T</em></sup></span> and <span class="math inline"><em>A</em><sup><em>T</em></sup><em>A</em></span> are the squares of the singular values! <u>_</u></p></li>
</ol>
<h1 id="pca">PCA</h1>
<p>Let <span class="math inline"><em>X</em></span> be a (centered) <span class="math inline"><em>m</em> × <em>n</em></span> data matrix. Then <span class="math inline">$\frac{1}{n-1}XX^T$</span> is the corresponding covariance matrix.</p>
<h2 id="section-10"></h2>
<p>If there are non-zero off-diagonal values in the covariance matrix, that means that the coordinates we picked have some redundancy. Let this notion sink in. There are many ways to think about this, but it all boils down to the fact that, if there are non-zero off-diagonal values in the cov. matrix, then the coordinates we're using are not orthogonal.</p>
<h3 id="section-11"></h3>
<p>Why does this matter? Because if we have orthogonal coordinates, we know that these are the directions of greates variation (hence "Principal Components"), and in a lucky scenario, we might even find that we need less dimensions to represent the data at hand. (For an excelent example of this, and a lecture much better than my notes, check <a href="https://www.youtube.com/watch?v=a9jdQGybYmE">this video</a> by Prof. J. Nathan Kutz)</p>
<h2 id="pcas-goal">PCA's goal</h2>
<p>Find a coordinate system where the covariance matrix is diagonal.</p>
<h2 id="approach-1">Approach 1</h2>
<h3 id="section-12"></h3>
<p><span class="math inline"><em>X</em><em>X</em><sup><em>T</em></sup></span> is square and symmetric. It is possible to do eigendecomposition on it:</p>
<h3 id="section-13"></h3>
<p>Now take the following transformation: <span class="math inline"><em>Y</em> = <em>S</em><sup><em>T</em></sup><em>X</em></span>. Recall that the columns of <span class="math inline"><em>S</em></span> are orthogonal, so <span class="math inline"><em>S</em><sup><em>T</em></sup> = <em>S</em><sup> − 1</sup></span>. Then, the covariance matrix in the new space is:</p>
<h3 id="section-14"></h3>
<p>And we know <span class="math inline"><em>Λ</em></span> is diagonal, so we successfully found a coordinate transform where off-diagonal values of the covariance matrix are 0, hence minimizing redundancy.</p>
<h3 id="section-15"></h3>
<p>Remember: to obtain <span class="math inline"><em>Y</em></span>, we hit <span class="math inline"><em>X</em></span> with <span class="math inline"><em>S</em><sup><em>T</em></sup></span>. What's the interpretation of this?</p>
<ol>
<li><p>If <span class="math inline"><em>Y</em> = <em>S</em><sup><em>T</em></sup><em>X</em></span>, then <span class="math inline"><em>X</em> = <em>S</em><em>Y</em></span>,</p></li>
<li><p>Which means that to "translate" from <span class="math inline"><em>Y</em></span> to <span class="math inline"><em>X</em></span> we use <span class="math inline"><em>S</em></span> as our change-of-basis matrix.</p></li>
<li><p>Conversely, doing <span class="math inline"><em>Y</em> = <em>S</em><sup><em>T</em></sup><em>X</em></span>, translates <span class="math inline"><em>X</em></span> into the column-space of <span class="math inline"><em>S</em></span>.</p></li>
<li><p>And the columns of <span class="math inline"><em>S</em></span> are the eigenvectors of $XX<sup>T</sup>$!</p></li>
</ol>
<h3 id="section-16"></h3>
<p>So, in summary, the coordinate system in which redundancy is minimized is precisely the one given by the eigenvectors of $XX<sup>T</sup>$!</p>
<p>(Speaking of redundancy, this last chunk is a good example of that, but writing it down like that does help consolidate things)</p>
<h2 id="approach-2">Approach 2</h2>
<h3 id="section-17"></h3>
<p>Same setting, but let's use the two following points:</p>
<ul>
<li><span class="math inline"><em>X</em> = <em>U</em><em>Σ</em><em>V</em><sup>*</sup></span></li>
<li><span class="math inline"><em>Y</em> = <em>U</em><sup>*</sup><em>X</em></span></li>
</ul>
<h3 id="section-18"></h3>
<p>In this case, the covariance matrix in the <span class="math inline"><em>Y</em></span> space is:</p>
<ol>
<li><p>Where I've repeated some of the steps with the parenthesis in different locations, to make things clearer.</p></li>
</ol>
<h3 id="section-20"></h3>
<p>You can easily see that <span class="math inline"><em>Σ</em> * <em>Σ</em><sup><em>T</em></sup></span> is diagonal (its last <span class="math inline">(<em>m</em> − <em>n</em>)</span> diagonal elements are going to be zero).</p>
<h3 id="section-21"></h3>
<p>Again, what's the meaning of this? Following the same rationale, we're saying that <span class="math inline"><em>U</em><sup>*</sup></span> is a change-of-basis matrix that takes us to a space where the covariance matrix is diagonal. If you paid attention, you'll notice that the columns of <span class="math inline"><em>U</em></span> are precisely the eigenvectors of <span class="math inline"><em>X</em><em>X</em><sup><em>T</em></sup></span> - the coordinate system we arrived at, in the previous section. So there's the connection between the two approaches. (You could have guessed it by the fact that we normally think of computing the eigendecomposition of <span class="math inline"><em>X</em><em>X</em><sup><em>T</em></sup></span> and <span class="math inline"><em>X</em><sup><em>T</em></sup><em>X</em></span>).</p>
<h2 id="tldr">TL;DR</h2>
<p>Given a data matrix <span class="math inline"><em>X</em></span>, the principal directions of variation in that data are given by the eigenvectors of its covariance matrix, and the magnitude of variation in each of those directions is given by the eigenvalues of the covariance matrix.</p>
