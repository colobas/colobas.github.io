<h1 id="section"></h1>
<p>In this post I'll do an ultra quick skim over Prof. Mikhail Belkin's awesome paper "Reconciling moder machine-learning with the bias-variance trade-off". <strong>All images in this post where taken from the paper.</strong></p>
<p>For a complete explanation of these ideas:</p>
<ul>
<li>Read <a href="https://arxiv.org/pdf/1812.11118.pdf">the paper</a></li>
<li>Watch <a href="http://www.fields.utoronto.ca/video-archive/static/2018/11/2509-19885/mergedvideo.ogv">this video lecture</a></li>
</ul>
<h1 id="section-1"></h1>
<p>This paper is a step in the direction of understanding why Deep Learning… works.</p>
<h1 id="section-2"></h1>
<p>The classical way in which a modeller picks the complexity of the model (s)he will fit is by bearing the following plot in mind: <img src="file:///images/u-shaped-curve.png" /></p>
<h2 id="section-3"></h2>
<p>Here it's easy to see that there's a point from which on increasing complexity translates to degrading test performance. In the classical scenario, model complexity is tuned to be close to the minimum of this U-shaped curve: the sweet-spot between having enough expressiveness to fit closely enough to the training data, and not being too expressive so as to not simply memorize the training data (what's commonly called <strong>overfitting</strong>).</p>
<h2 id="section-4"></h2>
<p>However, in the era of Deep Learning, the case is, more often than not, that we have extremely complex models - where the number of parameters is much larger than the number of training samples - which are almost perfectly fitted to training data, reaching very low training error, and are still able to generalize. This contradicts the classical view, and still requires a complete theoretical explanation. Prof. Belkin's work is a very interesting step in the direction of that explanation.</p>
<h2 id="section-5"></h2>
<p>The paper's central idea is that there is another regime, classically not considered, after the interpolation threshold of model complexity (for many models this just means having at least as many parameters as data points). From this point on, models are able to perfectly fit the training data, but their test risk starts decreasing again, which creates what the paper describes as a "double descent curve": <img src="file:///images/double-descent-curve.png" /></p>
<h3 id="section-6"></h3>
<p>But why would this happen? The first thing to notice is that, after the interpolation threshold, the training error basically drops to zero, regardless of the number of parameters: so clearly Empirical Risk Minimization stops being a proxy for the true performance. According to the paper, the answer lies in the appropriate definition of the inductive bias associated with each problem. They suggest that a function-space norm (i.e., a norm defined over a space of functions - e.g., the <span class="math inline"><em>l</em><sub>2</sub></span>-norm of the parameter vector of a parametric family) can be an appropriate proxy for the inductive bias. The punchline is that a vector with more elements can have a smaller norm than a vector with fewer elements, so if that norm is an actual measure of the simplicity (regularity) of the learned function, it's possible that we can access simpler (more regular) learned functions with more complex model families. The paper claims that using such a norm in this manner constitutes a form of Occam's razor.</p>
<h3 id="section-7"></h3>
<p>The paper presents empiricial evidence for these claims using a parametric model. They fit the model in the classical regime, and in the "modern" regime, varying the number of parameters <span class="math inline"><em>n</em></span>. For each <span class="math inline"><em>n</em></span>, given solutions which yield the same training error, they choose the one with the smallest <span class="math inline"><em>l</em><sub>2</sub></span>-norm. Doing so they obtain the predicted "double descent curve". Moreover, they plot the value of the solution's <span class="math inline"><em>l</em><sub>2</sub></span>-norm for each <span class="math inline"><em>n</em></span>, and it decreases as predicted after the interpolation threshold: <img src="file:///images/rff-belkin.png" /></p>
<p>I found these ideas extremely interesting, and invite everyone to read the paper.</p>
